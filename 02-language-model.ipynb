{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview\n",
    "### 定义\n",
    "- 标准定义\n",
    "\t- 对于语言序列，w<sub>1</sub>, w<sub>2</sub>,...,w<sub>n</sub>，语言模型技术差计算该序列的概率，即 P(w<sub>1</sub>, w<sub>2</sub>,...,w<sub>n</sub>)\n",
    "- 机器学习角度定义\n",
    "\t- 语言模型是对语句的概率分布的建模\n",
    "- 通俗解释\n",
    "\t- 判断个语音序列是否是正常语句，即是否是人话，如：P(我是中国人) > P(中国是我人)\n",
    "\n",
    "### 计算\n",
    "- 链式法则\n",
    "\t- P(w<sub>1</sub>, w<sub>2</sub>,...,w<sub>n</sub>) = P(w<sub>1</sub>)P(w<sub>2</sub>|w<sub>1</sub>),...,P(w<sub>n</sub>|w<sub>1</sub>,...,w<sub>n-1</sub>)\n",
    "\t- 如：P(我是中国人) = P(我)*P(是｜我)*P(中｜我是)*P(国｜我是中)*P(人｜我是中国)\n",
    "- Markov假设\n",
    "\t- 某个词出现的概率只依赖于前面的有限个词（如k个词）\n",
    "\t- P(w<sub>i</sub>｜w<sub>1</sub>,w<sub>2</sub>,...,w<sub>i-1</sub>) 约等于 P(w<sub>i</sub>|w<sub>i-k</sub>,...,w<sub>i-1</sub>)\n",
    "### 用途\n",
    "- NLG（自然语言生成）\n",
    "- LM是NLP的基石，能用于各种下游任务\n",
    "\t- 文本分类\n",
    "\t- 序列标注\n",
    "\t- 文本匹配\n",
    "\t- 语言识别\n",
    "- Model Pretraining for NLP（NLP预训练模型）\n",
    "\n",
    "## n-gram language model\n",
    "语言模型的基础\n",
    "### 概念\n",
    "在马尔可夫假设（Markov assumption）基础上，即假设当前词出现的概率只依赖于前n-1个词，可以得到\n",
    "> P(w<sub>i</sub>｜w<sub>1</sub>,w<sub>2</sub>,...,w<sub>i-1</sub>) = P(w<sub>i</sub>|w<sub>i-n+1</sub>,...,w<sub>i-1</sub>)\n",
    "\n",
    "基于上式，定义 n-gram 语言模型如下：\n",
    "- n = 1 unigram\n",
    "\t- P(w<sub>i</sub>｜w<sub>1</sub>, w<sub>2</sub>,...,w<sub>i-1</sub>) 约等于 P(w<sub>i</sub>)\n",
    "\t- P(我是中国人) = P(我)*P(是)*P(中)*P(国)*P(人)\n",
    "- n = 2 bigram\n",
    "\t- P(w<sub>i</sub>｜w<sub>1</sub>, w<sub>2</sub>,...,w<sub>i-1</sub>) 约等于 P(w<sub>i</sub>｜w<sub>i-1</sub>)\n",
    "\t- P(我是中国人) = P(我)*P(是|我)*P(中｜是)*P(国｜中)*P(人｜国)\n",
    "- n = 3 trigram\n",
    "\t- P(w<sub>i</sub>｜w<sub>1</sub>, w<sub>2</sub>,...,w<sub>i-1</sub>) 约等于 P(w<sub>i</sub>｜w<sub>i-2</sub>,w<sub>i-1</sub>)\n",
    "\t- P(我是中国人) = P(我)*P(是|我)*P(中｜我是)*P(国｜是中)*P(人｜中国)\n",
    "- ...\n",
    "\n",
    "### 计算\n",
    "- 如何计算P(w<sub>i</sub>｜w<sub>i-k</sub>, w<sub>2</sub>,...,w<sub>i-1</sub>) \n",
    "> P(w<sub>i</sub>｜w<sub>i-k</sub>, w<sub>2</sub>,...,w<sub>i-1</sub>) = P(w<sub>i-k</sub>,...,w<sub>i-1</sub>,w<sub>i</sub>) / P(w<sub>i-k</sub>,...,w<sub>i-1</sub>) =\n",
    "Count(w<sub>i-k</sub>,...,w<sub>i-1</sub>,w<sub>i</sub>) / Count(w<sub>i-k</sub>,...,w<sub>i-1</sub>)\n",
    "\n",
    "如：P(w<sub>i</sub>｜w<sub>i-1</sub>) = Count(w<sub>i-1</sub>,w<sub>i</sub>) / Count(w<sub>i-1</sub>)\n",
    "\n",
    "### 训练\n",
    "\n",
    "语句：\n",
    "- 中国加油\n",
    "- 我是中国人\n",
    "- 第一届中国国际进口博览会\n",
    "\n",
    "计算概率：\n",
    "P(国｜中) = Count(中国) / Count(中) = 3/3 = 1\n",
    "P(人｜国) = Count(国人) / Count(国) = 1/4 = 0.25\n",
    "P(人｜是) = Count(是人) / Count(人) = 0/1 = 0\n",
    "\n",
    "\n",
    "### 优缺点\n",
    "- 优点\n",
    "\t- 采用极大似然估计，参数易训练\n",
    "\t- 完全包含了前 n-1 个词的全部信息\n",
    "\t- 可解释性强，直观易理解\n",
    "\n",
    "- 缺点\n",
    "\t- 缺乏长期依赖，只能建模到前 n-1 个词\n",
    "\t- 数据稀疏，且随着 n 的增大，参数量过大会出现存储问题\n",
    "\t- 难免会出现OOV的问题\n",
    "\t- 单纯的基于统计频次，泛化能力差\n",
    "\n",
    "\n",
    "- 解决稀疏性问题\n",
    "\t- 平滑（Smoothing）法和回退（Backoff）法\n",
    "\n",
    "- OOV问题\n",
    "\t- 把出现次数少的词都归总为一个专有词： <UNK>\n",
    "\t- 使用subword\n",
    "\t\t- 中文词：搞快点 ---> 搞快、快点 ---> 搞、快、点\n",
    "\t\t- 英文词：perpetrator ---> per、erp、rpe、pet、etr、tra、rat、ato、tor\n",
    "\t- Hashing\n",
    "\t\t- 利用随机hashing，把很长的词表哈希到更短得多的稀疏向量表达\n",
    "### 评价\n",
    "- 外在评估法，即把训练好的语言模型放到下游任务中，看效果。\n",
    "\t- 文本分类\n",
    "\t- 序列标注\n",
    "\t-  文本匹配\n",
    "\n",
    "- 外在评估法难点\n",
    "\t- 非常耗时，需要大量时间\n",
    "\t- 较为依赖数据集，可能受数据集影响\n",
    "\n",
    "- 内在评估法\n",
    "\t- 困惑度（Perplexity）\n",
    "## deep learn language model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
