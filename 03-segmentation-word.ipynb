{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 什么是中文分词\r\n",
    "\r\n",
    "把一个个表达单独含义的个体分开\r\n",
    "\r\n",
    "## 分词方法\r\n",
    "\r\n",
    "1. 基于字符串匹配的分词方法\r\n",
    "\r\n",
    "   基于已有词典进行匹配\r\n",
    "2. 基于理解的分词方法\r\n",
    "\r\n",
    "3. 基于统计的分词方法\r\n",
    "   \r\n",
    "   在给定大量已经分词的文本的前提下，利用统计机器学习模型学习词语切分的规律（称为训练），从而实现对未知文本的切分。如：最大概率分词方法和最大熵分词方法\r\n",
    "\r\n",
    "   渐渐成为主流方法，常见模型：N元文法模型（N-gram） HMM 最大熵模型 条件随机场 神经网络模型\r\n",
    "\r\n",
    "## 分词工具\r\n",
    "- jieba\r\n",
    "- 哈工大LTP\r\n",
    "- NLPIR\r\n",
    "- THULAC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## jieba"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import jieba\r\n",
    "\r\n",
    "content = '南京市长江大桥'\r\n",
    "jieba.cut(content)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<generator object Tokenizer.cut at 0x000001E88860C408>\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.1 64-bit"
  },
  "interpreter": {
   "hash": "a7232e33e786dd771982417c23b8a9128432fc416a52932f41c9df54c1505944"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}